#!/bin/bash

#SBATCH --partition=gpu
#SBATCH --gpus=1
#SBATCH --job-name=LICO_training
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=10
#SBATCH --time=03:00:00
#SBATCH --mem=16000M
#SBATCH --output=output/slurm_output_%A.out

module purge
module load 2022
module load Anaconda3/2022.05

# activate the environment
source activate fact2024

export WANDB_API_KEY="5a3df675e39d05fd081fbd9e460b5a37ad9ccd85"

# Run 1: Visual prompting CLIP on CIFAR-10 with standard text prompt
code_dir="$HOME/uva_fact_2024"

# Standard constants
ARCH="resnet18"
DATASET="cifar100"

data="$code_dir/data"
DATA_DIR="$code_dir/data"
SAVE_DIR="$code_dir/cgc_log"
LOG_DIR="$code_dir/cgc_log"

LAMBDA=0.5

EPOCHS=100
BATCH_SIZE=128
LR=0.03
MOMENTUM=0.9
WEIGHT_DECAY=0.0001
PRINT_FREQ=10
DIST_BACKEND="nccl"
SEED=42

## Copy the data to the scratch-local
root=/scratch-local/$USER
mkdir -p $root
cp -r $data "$root/data"

echo "Running experiment on $dataset with $training_method $method and batch size $batch_size and seed $seed"
python $code_dir/train_eval_cgc.py \
	-a $ARCH \
	--epochs $EPOCHS \
	-b $BATCH_SIZE \
	--lr $LR \
	--momentum $MOMENTUM \
	--wd $WEIGHT_DECAY \
	-p $PRINT_FREQ \
	--seed $SEED \
	--save_dir $SAVE_DIR \
	--log_dir $LOG_DIR \
	--dataset $DATASET \
	--lambda $LAMBDA \
	$DATA_DIR

